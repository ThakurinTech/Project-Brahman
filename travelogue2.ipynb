{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:392: UserWarning: Tokenizer does not support real words, using fallback heuristic\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India' 'Kaladi' 'Kerala' 'Samsara' 'Kaveri' 'Varanasi' 'Ganga' 'Badarf'\n",
      " 'Kasi' 'Advaita' 'Horizon' 'Badari' 'Sankara' 'Iswara' 'Siva' 'Mandara'\n",
      " 'Rameswaram' 'Hithalaya' 'West' 'Tamala' 'Naraka' 'Manikarnika' 'Prayaga'\n",
      " 'Bharata' 'Vindhyas' 'Yamuna' 'Triveni' 'Mahishmati' 'Kashmir' 'Vidarbha'\n",
      " 'Bijlabindu' 'Hastinapura' 'Raghuvamsa' 'Narmada' 'Sisupala' 'Dethi'\n",
      " 'Ahavaniya' 'Anantanandagiri' 'Bombay' 'Hampi' 'Tungabhadra' 'Kanchi'\n",
      " 'Madras' 'Bangalore' 'Sringeri' 'Tretayuga' 'Avidya' 'Vidya' 'Kalpaka'\n",
      " 'Bapatla' 'Guntur' 'Amaruka' 'Brahmarandhra' 'Brahmaloka' 'Maharashtra'\n",
      " 'Srisaila' 'Kapila' 'Kailasa' 'Samadhi' 'Ashrama' 'Bala' 'Gokarna' 'Bali'\n",
      " 'Yajna' 'Mookambika' 'Sushumna' 'Manipura' 'Indra' 'Brahma' 'Vaikuntha'\n",
      " 'Kalahasti' 'Suvarnamukhi' 'Pundarikapura' 'Sahya' 'Chidambaram'\n",
      " 'Kanchipuram' 'Kalpataru' 'Sethu' 'South' 'Lanka' 'Asoka' 'Ravana' 'Setu'\n",
      " 'Mahasurapura' 'Karnataka' 'Pandya' 'Hasti' 'Andhra' 'Tamraparni'\n",
      " 'Pataliputra' 'Kapalikas' 'Dwaraka' 'Ujjayini' 'Saurashtra' 'Kamakoti'\n",
      " 'Nidhi' 'Bahlika' 'Surasena' 'Darada' 'Kuru' 'Panchala' 'Kamarupa'\n",
      " 'Kosala' 'Anga' 'Gauda' 'Kanada' 'Kshapanakas' 'Mysore' 'Gaddi' 'Ujjain'\n",
      " 'Badami' 'Mandukya' 'East' 'North' 'Akasa' 'Saradapitha' 'Kedara'\n",
      " 'Sringagiri' 'Sivaloka' 'Canton' 'Selukos' 'Syria' 'Egypt' 'Macedonia'\n",
      " 'Cyrene' 'Epirus' 'Dwarka' 'Totaka' 'Puri' 'Padmapada' 'Chennai'\n",
      " 'Mylapore' 'Puris' 'Sringen' 'Jyothi' 'Kedar' 'Kedarnath' 'Badrinath'\n",
      " 'Samadhisthan' 'Kailas' 'Alathur' 'Calcutta' 'Trichur' 'Vrishachala'\n",
      " 'Badarinarayan' 'Marathawada' 'Mahuripuri' 'Mahur' 'Mahuragad' 'Yavatmal'\n",
      " 'Sasalagrama' 'Sasilagrama' 'Kaaladi' 'Kal' 'Palikode' 'Fort'\n",
      " 'Trippunithura' 'Ponnur' 'London' 'Ullur' 'Trivandrum' 'Swarga'\n",
      " 'Devaloka' 'Sudhanvan' 'Jaina' 'Sivalingam' 'Purna' 'Vadakkunathan'\n",
      " 'Periyar' 'Gurukula' 'Garhyapatyagni' 'Ahavaniyagni' 'Lagna' 'Meru'\n",
      " 'Moksha' 'Sona' 'Kundina' 'Gokula' 'Vyaghrapura' 'Rama' 'Ganges' 'Dwaita'\n",
      " 'Tilaka' 'Munja' 'Kashaya' 'Sruti' 'Tripurds' 'Kutaja' 'Prakriti'\n",
      " 'Manasa' 'Reva' 'Himalayas']\n"
     ]
    }
   ],
   "source": [
    "#Book 2 Adi Shankracharya\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Load pre-trained tokenizer and model for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "\n",
    "# Define your data\n",
    "mydata = \"txt/Sankara-Digvijaya_images.txt\"\n",
    "\n",
    "# Initialize NER pipeline with modified aggregation_strategy\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"average\")\n",
    "\n",
    "# Chunk size for iterating over the text\n",
    "chunk_size = 1000\n",
    "\n",
    "# Initialize an empty list to store unique locations\n",
    "unique_locations = []\n",
    "\n",
    "# Write the locations to a CSV file\n",
    "with open(\"locations.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    fieldnames = ['word', 'start', 'end']  # Define the field names for the CSV file\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Read text from file and process in chunks\n",
    "    with open(mydata, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "        # Preprocess the text\n",
    "        text = text.replace('\\n', ' ').replace('.', ' ').replace(',', ' ')\n",
    "\n",
    "        # Process text in chunks\n",
    "        for i in range(0, len(text), chunk_size):\n",
    "            chunk = text[i:i+chunk_size]\n",
    "            # Perform NER on the chunk\n",
    "            output = nlp(chunk)\n",
    "            # Write the locations to the CSV file\n",
    "            for entity in output:\n",
    "                if entity['entity_group'] == 'LOC':  # Check if the entity is a location\n",
    "                    location = entity['word'].strip()\n",
    "                    if location.isalpha() and location.istitle() and len(location) > 2:  # Filter by capitalization, length, and alphabetic characters\n",
    "                        writer.writerow({'word': location, 'start': entity['start'], 'end': entity['end']})\n",
    "                        unique_locations.append(location)  # Add location to the list\n",
    "\n",
    "# Convert the list of unique locations to a DataFrame\n",
    "df = pd.DataFrame({'A': unique_locations})\n",
    "\n",
    "# Get unique values of 'A' column (locations)\n",
    "unique_values = df['A'].unique()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINDING LOCATIONS\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "# Replace this with the path to your input CSV file\n",
    "input_csv_path = 'secondbook.csv'\n",
    "# Output CSV file path\n",
    "output_csv_path = 'wikidata_locations2_with_geo.csv'\n",
    "\n",
    "def get_geolocation(place_name):\n",
    "    # Wikidata endpoint for SPARQL queries\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    # SPARQL query to get geolocation for a place name\n",
    "    query = f\"\"\"\n",
    "    SELECT ?place ?placeLabel ?location WHERE {{\n",
    "      ?place ?label \"{place_name}\"@en.\n",
    "      ?place wdt:P625 ?location.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }}\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'MyApp/1.0 (myemail@example.com)', 'Accept': 'application/json'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params={'query': query, 'format': 'json'})\n",
    "        response.raise_for_status()  # Raises HTTPError, if one occurred\n",
    "        data = response.json()\n",
    "        results = data['results']['bindings']\n",
    "        if results:\n",
    "            location = results[0]['location']['value']\n",
    "            # Extracting latitude and longitude from the point string\n",
    "            lon, lat = location.replace('Point(', '').replace(')', '').split(' ')\n",
    "            return lat, lon\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching geolocation for {place_name}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "# Read place names from CSV\n",
    "places = []\n",
    "with open(input_csv_path, mode='r', encoding='utf-8') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)  # Skip header\n",
    "    for row in reader:\n",
    "        places.append(row[0])  # Assuming place name is in the first column\n",
    "\n",
    "# Fetch geolocations and write to a new CSV\n",
    "with open(output_csv_path, mode='w', encoding='utf-8', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(['Place Name', 'Latitude', 'Longitude'])\n",
    "    for place in places:\n",
    "        lat, lon = get_geolocation(place)\n",
    "        if lat and lon:\n",
    "            writer.writerow([place, lat, lon])\n",
    "            print(f\"Geolocation for {place}: {lat}, {lon}\")\n",
    "        else:\n",
    "            print(f\"No geolocation found for {place}\")\n",
    "\n",
    "print(\"\\nDone writing geolocations to CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Given list of codes\n",
    "locations = ['India', 'Kaladi', 'Kerala', 'Kaveri', 'Varanasi', 'Kasi', 'Advaita', 'Horizon', 'Badarinath', 'Mandara', 'Rameswaram', 'Hithalaya', 'Tamala', 'Naraka', 'Manikarnika', 'Prayaga', 'Bharata', 'Vindhyas', 'Triveni', 'Mahishmati', 'Kashmir', 'Vidarbha', 'Bijlabindu', 'Hastinapura', 'Raghuvamsa', 'Narmada', 'Sisupala', 'Dethi', 'Ahavaniya', 'Anantanandagiri', 'Bombay', 'Hampi', 'Tungabhadra', 'Kanchi', 'Madras', 'Bangalore', 'Sringeri', 'Tretayuga', 'Avidya', 'Vidya', 'Kalpaka', 'Bapatla', 'Guntur', 'Amaruka', 'Brahmarandhra', 'Brahmaloka', 'Maharashtra', 'Srisaila', 'Kapila', 'Kailasa', 'Ashrama', 'Bala', 'Gokarna', 'Bali', 'Yajna', 'Mookambika', 'Manipura', 'Kalahasti', 'Suvarnamukhi', 'Pundarikapura', 'Sahya', 'Chidambaram', 'Kanchipuram', 'Kalpataru', 'Sethu', 'Lanka', 'Mahasurapura', 'Karnataka', 'Pandya', 'Hasti', 'Andhra', 'Tamraparni', 'Pataliputra', 'Kapalikas', 'Dwaraka', 'Ujjayini', 'Saurashtra', 'Kamakoti', 'Nidhi', 'Bahlika', 'Surasena', 'Darada', 'Kuru', 'Panchala', 'Kamarupa', 'Kosala', 'Anga', 'Gauda', 'Kanada', 'Kshapanakas', 'Mysore', 'Gaddi', 'Ujjain', 'Badami', 'Mandukya', 'Saradapitha', 'Kedara', 'Sringagiri', 'Selukos', 'Syria', 'Egypt', 'Macedonia', 'Cyrene', 'Epirus', 'Dwarka', 'Totaka', 'Puri', 'Padmapada', 'Chennai', 'Mylapore', 'Puris', 'Sringen', 'Jyothi', 'Kedar', 'Kedarnath', 'Badrinath', 'Samadhisthan', 'Kailash', 'Alathur', 'Calcutta', 'Trichur', 'Vrishachala', 'Badarinarayan', 'Marathawada', 'Mahuripuri', 'Mahur', 'Mahuragad', 'Yavatmal', 'Sasalagrama', 'Sasilagrama', 'Kaaladi', 'Kal', 'Palikode', 'Trippunithura', 'Ponnur', 'London', 'Ullur', 'Trivandrum', 'Devaloka', 'Sudhanvan', 'Jaina', 'Sivalingam', 'Purna', 'Vadakkunathan', 'Periyar', 'Gurukula', 'Garhyapatyagni', 'Ahavaniyagni', 'Lagna', 'Meru', 'Moksha', 'Sona', 'Kundina', 'Gokula', 'Vyaghrapura', 'Dwaita', 'Tilaka', 'Munja', 'Kashaya', 'Sruti', 'Tripurds', 'Kutaja', 'Manasa', 'Reva', 'Himalayas']\n",
    "\n",
    "# Define the file name\n",
    "file_name = 'place_and_locations2.csv'\n",
    "\n",
    "# Write the codes to a CSV file\n",
    "with open(file_name, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    for location in locations:\n",
    "        writer.writerow([location])\n",
    "\n",
    "print(f\"CSV file '{file_name}' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "locations = {\n",
    "    \"Kaladi\": (10.819338, 76.002973),\n",
    "    \"Kerala\": (10.0, 76.3),\n",
    "    \"Kaveri\": (11.370345, 79.828105),\n",
    "    \"Varanasi\": (25.318888888, 83.012777777),\n",
    "    \"Mandara\": (22.9087, 88.11603),\n",
    "    \"Rameswaram\": (9.28, 79.3),\n",
    "    \"Prayaga\": (25.45, 81.85),\n",
    "    \"Bharata\": (22.8, 83.0),\n",
    "    \"Mahishmati\": (22.176944444, 75.585555555),\n",
    "    \"Kashmir\": (34.5, 76.0),\n",
    "    \"Vidarbha\": (21.15, 79.09),\n",
    "    \"Bombay\": (19.075833333, 72.8775),\n",
    "    \"Hampi\": (15.334547, 76.462162),\n",
    "    \"Madras\": (13.0825, 80.275),\n",
    "    \"Bangalore\": (12.9791198, 77.5912997),\n",
    "    \"Sringeri\": (13.42, 75.26),\n",
    "    \"Bapatla\": (15.905, 80.468),\n",
    "    \"Guntur\": (16.3, 80.45),\n",
    "    \"Maharashtra\": (18.97, 72.82),\n",
    "    \"Gokarna\": (14.55, 74.31667),\n",
    "    \"Manipura\": (13.419166666, 75.149444444),\n",
    "    \"Kalahasti\": (13.75, 79.7),\n",
    "    \"Chidambaram\": (11.399444444, 79.693611111),\n",
    "    \"Kanchipuram\": (12.830833333, 79.707777777),\n",
    "    \"Karnataka\": (15.0, 76.0),\n",
    "    \"Hasti\": (30.635555555, 79.755),\n",
    "    \"Andhra\": (16.52, 79.5),\n",
    "    \"Pataliputra\": (25.61, 85.141388888),\n",
    "    \"Dwaraka\": (22.240277777, 68.968611111),\n",
    "    \"Ujjayini\": (23.182777777, 75.777222222),\n",
    "    \"Saurashtra\": (22.3, 70.7833),\n",
    "    \"Mysore\": (12.308611111, 76.653055555),\n",
    "    \"Ujjain\": (23.182777777, 75.777222222),\n",
    "    \"Badami\": (15.91495, 75.67683),\n",
    "    \"Dwarka\": (22.240277777, 68.968611111),\n",
    "    \"Puri\": (19.8, 85.816666666),\n",
    "    \"Chennai\": (13.0825, 80.275),\n",
    "    \"Mylapore\": (13.0336, 80.2687),\n",
    "    \"Kedarnath\": (30.73333333, 79.06666667),\n",
    "    \"Badrinath\": (30.742425, 79.496038888),  # Corrected duplicate key\n",
    "    \"Kailash\": (31.066944444, 81.312777777),\n",
    "    \"Alathur\": (10.648055555, 76.538333333),\n",
    "    \"Calcutta\": (22.5726723, 88.3638815),\n",
    "    \"Yavatmal\": (20.4, 78.133333333),\n",
    "    \"Ponnur\": (16.0667, 80.5667),\n",
    "    \"Ullur\": (10.975, 79.401388888),\n",
    "    \"Trivandrum\": (8.4875, 76.9525),\n",
    "    \"Purna\": (21.0958, 76.01),\n",
    "    \"Kundina\": (20.55, 77.45),\n",
    "    \"Manasa\": (24.48, 75.15),\n",
    "    \"Himalayas\": (29.0, 84.0)\n",
    "}\n",
    "# Create a map centered around the midpoint of the journey\n",
    "map = folium.Map(location=[38.6270, -90.1994], zoom_start=5)\n",
    "\n",
    "# Adding markers to the map\n",
    "for city, coords in locations.items():\n",
    "    folium.Marker(\n",
    "        location=coords,\n",
    "        popup=city,\n",
    "        icon=folium.Icon(color='blue')  # Example of using a blue marker\n",
    "    ).add_to(map)\n",
    "\n",
    "# Extracting the coordinates from the locations dictionary\n",
    "polyline_coords = list(locations.values())\n",
    "\n",
    "# Add lines between the locations\n",
    "folium.PolyLine(\n",
    "    locations=polyline_coords,\n",
    "    color=\"blue\"\n",
    ").add_to(map)\n",
    "\n",
    "# Display the map\n",
    "map.save('travel_heatmap_book2.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
